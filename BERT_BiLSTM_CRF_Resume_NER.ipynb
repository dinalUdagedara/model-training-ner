{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# BERT-BiLSTM-CRF for Resume NER (rule-compliant)\n",
        "\n",
        "Run cells in order.\n",
        "\n",
        "**Run in Cursor:** Use a **local Python kernel** (not Colab): click the kernel name top-right → select your project’s Python (e.g. conda/venv). Then run cells with **Shift+Enter** or ▶. The JSON file is loaded from the same folder as this notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded 220 resumes\n"
          ]
        }
      ],
      "source": [
        "# 1) Load data — set DATA_PATH to your JSON file path if needed\n",
        "import json\n",
        "import os\n",
        "\n",
        "DATA_PATH = \"entity_recognition_in_resumes.json\"\n",
        "if not os.path.exists(DATA_PATH):\n",
        "    DATA_PATH = \"/content/drive/My Drive/DATASETS/entity_recognition_in_resumes.json\"\n",
        "if not os.path.exists(DATA_PATH):\n",
        "    raise FileNotFoundError(\"JSON file not found. Set DATA_PATH in this cell to the path of entity_recognition_in_resumes.json and re-run.\")\n",
        "\n",
        "data = []\n",
        "with open(DATA_PATH, \"r\", encoding=\"utf-8\") as f:\n",
        "    for line in f:\n",
        "        line = line.strip()\n",
        "        if line:\n",
        "            data.append(json.loads(line))\n",
        "print(f\"Loaded {len(data)} resumes\")\n",
        "\n",
        "LABEL_MAPPING = {\n",
        "    \"Name\": \"NAME\", \"Email Address\": \"EMAIL\", \"Skills\": \"SKILL\", \"Designation\": \"OCCUPATION\",\n",
        "    \"Degree\": \"EDUCATION\", \"College Name\": \"EDUCATION\", \"Graduation Year\": \"EDUCATION\",\n",
        "    \"Companies worked at\": \"EXPERIENCE\", \"Years of Experience\": \"EXPERIENCE\", \"Location\": \"O\", \"UNKNOWN\": \"O\",\n",
        "}\n",
        "for item in data:\n",
        "    for ann in item.get(\"annotation\", []):\n",
        "        ann[\"label\"] = [LABEL_MAPPING.get(l, \"O\") for l in ann[\"label\"]]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Colab only – use if upload widget didn't work:** Run the cell below to mount Google Drive. Put `entity_recognition_in_resumes.json` in your Drive (e.g. in My Drive), then in **cell 1** set `DATA_PATH = '/content/drive/MyDrive/entity_recognition_in_resumes.json'` (or the path where you put it) and re-run cell 1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Not in Colab – skip this cell.\n"
          ]
        }
      ],
      "source": [
        "# Colab: Mount Google Drive (run this, then set DATA_PATH in cell 1 to your file path in Drive)\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount(\"/content/drive\")\n",
        "    print(\"Drive mounted. Put entity_recognition_in_resumes.json in My Drive, then set DATA_PATH in cell 1 and re-run it.\")\n",
        "except ImportError:\n",
        "    print(\"Not in Colab – skip this cell.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train 176 Val 22 Test 22\n"
          ]
        }
      ],
      "source": [
        "# 2) Build train/val/test from JSON with fixed create_bio_tags (no B-O / I-O)\n",
        "import re\n",
        "import random\n",
        "\n",
        "def tokenize_with_positions(text):\n",
        "    return [(m.group(), m.start(), m.end()) for m in re.finditer(r\"\\S+\", text)]\n",
        "\n",
        "def create_bio_tags_fixed(tokens, annotations):\n",
        "    bio = [\"O\"] * len(tokens)\n",
        "    for ann in annotations:\n",
        "        if not ann.get(\"label\") or ann[\"label\"][0] == \"O\":\n",
        "            continue\n",
        "        entity = ann[\"label\"][0]\n",
        "        for pt in ann.get(\"points\", []):\n",
        "            s, e = pt[\"start\"], pt[\"end\"]\n",
        "            first = True\n",
        "            for i, (_, ts, te) in enumerate(tokens):\n",
        "                if te <= s or ts >= e: continue\n",
        "                bio[i] = f\"B-{entity}\" if first else f\"I-{entity}\"\n",
        "                first = False\n",
        "    return bio\n",
        "\n",
        "all_sents, all_labels = [], []\n",
        "for item in data:\n",
        "    content = item.get(\"content\", \"\")\n",
        "    anns = item.get(\"annotation\", [])\n",
        "    if not content or not anns: continue\n",
        "    toks = tokenize_with_positions(content)\n",
        "    if not toks: continue\n",
        "    labs = create_bio_tags_fixed(toks, anns)\n",
        "    all_sents.append([t[0] for t in toks])\n",
        "    all_labels.append(labs)\n",
        "\n",
        "n = len(all_sents)\n",
        "random.seed(42)\n",
        "idx = list(range(n)); random.shuffle(idx)\n",
        "n_train, n_val = int(0.8 * n), int(0.1 * n)\n",
        "train_sents = [all_sents[i] for i in idx[:n_train]]\n",
        "train_labels = [all_labels[i] for i in idx[:n_train]]\n",
        "val_sents   = [all_sents[i] for i in idx[n_train:n_train+n_val]]\n",
        "val_labels  = [all_labels[i] for i in idx[n_train:n_train+n_val]]\n",
        "test_sents  = [all_sents[i] for i in idx[n_train+n_val:]]\n",
        "test_labels = [all_labels[i] for i in idx[n_train+n_val:]]\n",
        "print(f\"Train {len(train_sents)} Val {len(val_sents)} Test {len(test_sents)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "python(8270) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m26.0\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "# 3) Install deps (run once; skip if already installed)\n",
        "!pip install -q torch transformers pytorch-crf seqeval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Datasets ready\n"
          ]
        }
      ],
      "source": [
        "# 4) BERT tokenizer + label alignment and dataset\n",
        "from transformers import BertTokenizer\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch\n",
        "\n",
        "TAGS = [\"O\",\"B-NAME\",\"I-NAME\",\"B-EMAIL\",\"I-EMAIL\",\"B-SKILL\",\"I-SKILL\",\"B-OCCUPATION\",\"I-OCCUPATION\",\"B-EXPERIENCE\",\"I-EXPERIENCE\",\"B-EDUCATION\",\"I-EDUCATION\"]\n",
        "LABEL2ID = {t:i for i,t in enumerate(TAGS)}\n",
        "ID2LABEL = {i:t for i,t in enumerate(TAGS)}\n",
        "NUM_LABELS = len(TAGS)\n",
        "\n",
        "def align_to_bert(words, word_labels, tokenizer, max_len=512):\n",
        "    first_idx, toks = [], [\"[CLS]\"]\n",
        "    for w in words:\n",
        "        p = tokenizer.tokenize(w) or [tokenizer.unk_token]\n",
        "        first_idx.append(len(toks))\n",
        "        toks.extend(p)\n",
        "    toks.append(\"[SEP]\")\n",
        "    ids = tokenizer.convert_tokens_to_ids(toks)\n",
        "    mask = [1]*len(ids)\n",
        "    aligned = [-100]*len(ids)\n",
        "    for pos, lab in zip(first_idx, word_labels):\n",
        "        if pos < len(aligned): aligned[pos] = LABEL2ID.get(lab, 0)\n",
        "    if len(ids) > max_len:\n",
        "        ids, mask, aligned = ids[:max_len-1]+[tokenizer.sep_token_id], mask[:max_len-1]+[1], aligned[:max_len-1]+[-100]\n",
        "    return ids, mask, aligned\n",
        "\n",
        "class BertNERDataset(Dataset):\n",
        "    def __init__(self, sents, labels, tokenizer, max_len=512):\n",
        "        self.samples = [align_to_bert(w, l, tokenizer, max_len) for w, l in zip(sents, labels) if len(w)==len(l)]\n",
        "    def __len__(self): return len(self.samples)\n",
        "    def __getitem__(self, i): return self.samples[i]\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "train_ds = BertNERDataset(train_sents, train_labels, tokenizer)\n",
        "val_ds   = BertNERDataset(val_sents, val_labels, tokenizer)\n",
        "\n",
        "def collate(batch):\n",
        "    max_l = max(len(b[0]) for b in batch)\n",
        "    pad = 0\n",
        "    return (\n",
        "        torch.tensor([b[0]+[pad]*(max_l-len(b[0])) for b in batch], dtype=torch.long),\n",
        "        torch.tensor([b[1]+[0]*(max_l-len(b[1])) for b in batch], dtype=torch.long),\n",
        "        torch.tensor([b[2]+[-100]*(max_l-len(b[2])) for b in batch], dtype=torch.long),\n",
        "    )\n",
        "train_loader = DataLoader(train_ds, batch_size=8, shuffle=True, collate_fn=collate)\n",
        "val_loader   = DataLoader(val_ds, batch_size=8, collate_fn=collate)\n",
        "print(\"Datasets ready\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model on mps\n"
          ]
        }
      ],
      "source": [
        "# 5) BERT-BiLSTM-CRF model\n",
        "import torch.nn as nn\n",
        "from transformers import BertModel\n",
        "from torchcrf import CRF\n",
        "\n",
        "class BertBiLSTMCRF(nn.Module):\n",
        "    def __init__(self, bert_name=\"bert-base-uncased\", hidden_dim=256, num_labels=NUM_LABELS, dropout=0.3):\n",
        "        super().__init__()\n",
        "        self.bert = BertModel.from_pretrained(bert_name)\n",
        "        self.lstm = nn.LSTM(self.bert.config.hidden_size, hidden_dim//2, num_layers=1, bidirectional=True, batch_first=True)\n",
        "        self.drop = nn.Dropout(dropout)\n",
        "        self.fc = nn.Linear(hidden_dim, num_labels)\n",
        "        self.crf = CRF(num_labels, batch_first=True)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, labels=None):\n",
        "        out = self.bert(input_ids=input_ids, attention_mask=attention_mask).last_hidden_state\n",
        "        out, _ = self.lstm(self.drop(out))\n",
        "        emissions = self.fc(self.drop(out))\n",
        "        mask_b = attention_mask.bool()  # boolean avoids torch.where uint8 deprecation in pytorch-crf\n",
        "        if labels is not None:\n",
        "            # CRF expects indices in [0, num_labels-1]; replace padding -100 with 0 (mask ignores those)\n",
        "            labels = labels.clone().masked_fill(labels == -100, 0)\n",
        "            return -self.crf(emissions, labels, mask=mask_b, reduction=\"mean\")\n",
        "        return self.crf.decode(emissions, mask=mask_b)\n",
        "\n",
        "# Prefer GPU (CUDA or Apple Silicon MPS), else CPU\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "elif hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n",
        "    device = torch.device(\"mps\")  # Apple Silicon GPU\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "model = BertBiLSTMCRF().to(device)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
        "print(f\"Model on {device}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/15 Loss: 506.0813\n",
            "Epoch 2/15 Loss: 212.6201\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[26]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      8\u001b[39m optimizer.zero_grad()\n\u001b[32m      9\u001b[39m loss = model(inp, mask, lab)\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     11\u001b[39m torch.nn.utils.clip_grad_norm_(model.parameters(), \u001b[32m1.0\u001b[39m)\n\u001b[32m     12\u001b[39m optimizer.step()\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.3/lib/python3.12/site-packages/torch/_tensor.py:648\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    638\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    639\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    640\u001b[39m         Tensor.backward,\n\u001b[32m    641\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    646\u001b[39m         inputs=inputs,\n\u001b[32m    647\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m648\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    650\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.3/lib/python3.12/site-packages/torch/autograd/__init__.py:353\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    348\u001b[39m     retain_graph = create_graph\n\u001b[32m    350\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m353\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.3/lib/python3.12/site-packages/torch/autograd/graph.py:824\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    822\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    823\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m824\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    825\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    826\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    827\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    828\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
            "\u001b[31mKeyboardInterrupt\u001b[39m: "
          ]
        }
      ],
      "source": [
        "# 6) Train (more epochs help with small data; gradient clipping stabilizes)\n",
        "EPOCHS = 15\n",
        "for epoch in range(EPOCHS):\n",
        "    model.train()\n",
        "    total = 0\n",
        "    for inp, mask, lab in train_loader:\n",
        "        inp, mask, lab = inp.to(device), mask.to(device), lab.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        loss = model(inp, mask, lab)\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        optimizer.step()\n",
        "        total += loss.item()\n",
        "    print(f\"Epoch {epoch+1}/{EPOCHS} Loss: {total/len(train_loader):.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "   EDUCATION       0.00      0.00      0.00        43\n",
            "       EMAIL       0.00      0.00      0.00        27\n",
            "  EXPERIENCE       0.00      0.00      0.00        46\n",
            "        NAME       0.00      0.00      0.00        23\n",
            "  OCCUPATION       0.00      0.00      0.00        37\n",
            "       SKILL       0.00      0.00      0.00        15\n",
            "\n",
            "   micro avg       0.00      0.00      0.00       191\n",
            "   macro avg       0.00      0.00      0.00       191\n",
            "weighted avg       0.00      0.00      0.00       191\n",
            "\n",
            "F1: 0.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/dinalbandara/.pyenv/versions/3.12.3/lib/python3.12/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/Users/dinalbandara/.pyenv/versions/3.12.3/lib/python3.12/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ],
      "source": [
        "# 7) Evaluate with seqeval\n",
        "from seqeval.metrics import classification_report, f1_score\n",
        "\n",
        "model.eval()\n",
        "true_all, pred_all = [], []\n",
        "with torch.no_grad():\n",
        "    for inp, mask, labels in val_loader:\n",
        "        inp, mask = inp.to(device), mask.to(device)\n",
        "        preds = model(inp, mask)\n",
        "        for b in range(inp.size(0)):\n",
        "            m, labs = mask[b].cpu(), labels[b].cpu()\n",
        "            pred_b = preds[b]\n",
        "            tlist, plist = [], []\n",
        "            pos = 0\n",
        "            for i in range(m.size(0)):\n",
        "                if m[i].item()==0: break\n",
        "                p = ID2LABEL[pred_b[pos]] if pos < len(pred_b) and pred_b[pos] < NUM_LABELS else \"O\"\n",
        "                pos += 1\n",
        "                if labs[i].item() == -100: continue\n",
        "                tlist.append(ID2LABEL[labs[i].item()])\n",
        "                plist.append(p)\n",
        "            if tlist and plist:\n",
        "                true_all.append(tlist)\n",
        "                pred_all.append(plist)\n",
        "\n",
        "print(classification_report(true_all, pred_all, zero_division=0))\n",
        "print(\"F1:\", f1_score(true_all, pred_all, zero_division=0))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
