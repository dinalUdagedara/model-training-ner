{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Resume NER with BiLSTM-CRF from Scratch (Path 2 – FYP)\n",
        "\n",
        "This notebook implements **Path 2**: a **BiLSTM + CRF** model for NER **built from scratch** (no pre-trained transformer). It uses **Word2Vec** embeddings trained on your resume corpus and the same entity types: **NAME**, **EMAIL**, **SKILL**, **OCCUPATION**, **EDUCATION**, **EXPERIENCE**.\n",
        "\n",
        "- **Feature engineering:** Word embeddings (Word2Vec) + optional capitalization.\n",
        "- **Model:** Embedding → BiLSTM → Linear → CRF.\n",
        "- **Training:** Full training loop with CRF loss."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Dependencies\n",
        "\n",
        "Run once. (No `transformers`; uses `gensim` for Word2Vec.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install -q torch pytorch-crf seqeval gensim"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Mount Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Data loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "import os\n",
        "\n",
        "_drive_base = \"/content/drive/MyDrive\" if os.path.exists(\"/content/drive/MyDrive\") else \"/content/drive/My Drive\"\n",
        "# Prefer larger merged dataset (original + LLM + Sri Lanka tech) for better accuracy; fallback to smaller file\n",
        "for _name in [\"merged_1030_plus_all_llm_plus_sri_lanka_tech.jsonl\", \"merged_1030_plus_all_llm.jsonl\", \"merged_resume_ner_with_llm.json\", \"merged_resume_ner.json\"]:\n",
        "    DATA_PATH = os.path.join(_drive_base, _name)\n",
        "    if os.path.exists(DATA_PATH):\n",
        "        break\n",
        "else:\n",
        "    raise FileNotFoundError(\"No data file found. Place merged_1030_plus_all_llm_plus_sri_lanka_tech.jsonl or merged_1030_plus_all_llm.jsonl in My Drive root.\")\n",
        "\n",
        "data = []\n",
        "with open(DATA_PATH, \"r\", encoding=\"utf-8\") as f:\n",
        "    for line in f:\n",
        "        line = line.strip()\n",
        "        if line:\n",
        "            data.append(json.loads(line))\n",
        "print(f\"Loaded {len(data)} resumes from {os.path.basename(DATA_PATH)}\")\n",
        "\n",
        "LABEL_MAPPING = {\n",
        "    \"Name\": \"NAME\", \"Email Address\": \"EMAIL\", \"Skills\": \"SKILL\", \"Designation\": \"OCCUPATION\",\n",
        "    \"Degree\": \"EDUCATION\", \"College Name\": \"EDUCATION\", \"Graduation Year\": \"EDUCATION\",\n",
        "    \"Companies worked at\": \"EXPERIENCE\", \"Years of Experience\": \"EXPERIENCE\", \"Location\": \"O\", \"UNKNOWN\": \"O\",\n",
        "    \"NAME\": \"NAME\", \"EMAIL\": \"EMAIL\", \"SKILL\": \"SKILL\", \"OCCUPATION\": \"OCCUPATION\", \"EDUCATION\": \"EDUCATION\", \"EXPERIENCE\": \"EXPERIENCE\", \"O\": \"O\",\n",
        "}\n",
        "for item in data:\n",
        "    for ann in item.get(\"annotation\", []):\n",
        "        ann[\"label\"] = [LABEL_MAPPING.get(l, \"O\") for l in ann[\"label\"]]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Preprocessing and train/val/test split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import re\n",
        "import random\n",
        "\n",
        "def tokenize_with_positions(text):\n",
        "    return [(m.group(), m.start(), m.end()) for m in re.finditer(r\"\\S+\", text)]\n",
        "\n",
        "def create_bio_tags_fixed(tokens, annotations):\n",
        "    bio = [\"O\"] * len(tokens)\n",
        "    for ann in annotations:\n",
        "        if not ann.get(\"label\") or ann[\"label\"][0] == \"O\":\n",
        "            continue\n",
        "        entity = ann[\"label\"][0]\n",
        "        for pt in ann.get(\"points\", []):\n",
        "            s, e = pt[\"start\"], pt[\"end\"]\n",
        "            first = True\n",
        "            for i, (_, ts, te) in enumerate(tokens):\n",
        "                if te <= s or ts >= e:\n",
        "                    continue\n",
        "                bio[i] = f\"B-{entity}\" if first else f\"I-{entity}\"\n",
        "                first = False\n",
        "    return bio\n",
        "\n",
        "all_sents, all_labels = [], []\n",
        "for item in data:\n",
        "    content = item.get(\"content\", \"\")\n",
        "    anns = item.get(\"annotation\", [])\n",
        "    if not content or not anns:\n",
        "        continue\n",
        "    toks = tokenize_with_positions(content)\n",
        "    if not toks:\n",
        "        continue\n",
        "    labs = create_bio_tags_fixed(toks, anns)\n",
        "    all_sents.append([t[0] for t in toks])\n",
        "    all_labels.append(labs)\n",
        "\n",
        "n = len(all_sents)\n",
        "random.seed(42)\n",
        "idx = list(range(n))\n",
        "random.shuffle(idx)\n",
        "n_train, n_val = int(0.8 * n), int(0.1 * n)\n",
        "train_sents = [all_sents[i] for i in idx[:n_train]]\n",
        "train_labels = [all_labels[i] for i in idx[:n_train]]\n",
        "val_sents = [all_sents[i] for i in idx[n_train : n_train + n_val]]\n",
        "val_labels = [all_labels[i] for i in idx[n_train : n_train + n_val]]\n",
        "test_sents = [all_sents[i] for i in idx[n_train + n_val :]]\n",
        "test_labels = [all_labels[i] for i in idx[n_train + n_val :]]\n",
        "print(f\"Train {len(train_sents)} Val {len(val_sents)} Test {len(test_sents)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Word embeddings (Word2Vec) and vocabulary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from gensim.models import Word2Vec\n",
        "import numpy as np\n",
        "\n",
        "EMBED_DIM = 256\n",
        "W2V_MIN_COUNT = 1\n",
        "W2V_WINDOW = 6\n",
        "W2V_EPOCHS = 35\n",
        "\n",
        "w2v = Word2Vec(sentences=all_sents, vector_size=EMBED_DIM, window=W2V_WINDOW, min_count=W2V_MIN_COUNT, epochs=W2V_EPOCHS, workers=4)\n",
        "\n",
        "PAD_TOKEN = \"<PAD>\"\n",
        "UNK_TOKEN = \"<UNK>\"\n",
        "word2id = {PAD_TOKEN: 0, UNK_TOKEN: 1}\n",
        "for w in w2v.wv.key_to_index:\n",
        "    if w not in word2id:\n",
        "        word2id[w] = len(word2id)\n",
        "vocab_size = len(word2id)\n",
        "\n",
        "embedding_matrix = np.zeros((vocab_size, EMBED_DIM), dtype=np.float32)\n",
        "for w, i in word2id.items():\n",
        "    if w in (PAD_TOKEN, UNK_TOKEN):\n",
        "        continue\n",
        "    if w in w2v.wv:\n",
        "        embedding_matrix[i] = w2v.wv[w]\n",
        "    else:\n",
        "        embedding_matrix[i] = np.random.normal(0, 0.1, EMBED_DIM)\n",
        "\n",
        "print(f\"Vocab size: {vocab_size}, Embed dim: {EMBED_DIM}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Labels and dataset (word-level, padded)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "TAGS = [\"O\", \"B-NAME\", \"I-NAME\", \"B-EMAIL\", \"I-EMAIL\", \"B-SKILL\", \"I-SKILL\", \"B-OCCUPATION\", \"I-OCCUPATION\", \"B-EXPERIENCE\", \"I-EXPERIENCE\", \"B-EDUCATION\", \"I-EDUCATION\"]\n",
        "LABEL2ID = {t: i for i, t in enumerate(TAGS)}\n",
        "ID2LABEL = {i: t for i, t in enumerate(TAGS)}\n",
        "NUM_LABELS = len(TAGS)\n",
        "\n",
        "def sent_to_ids(words, word2id, max_len=768):\n",
        "    ids = [word2id.get(w, word2id[UNK_TOKEN]) for w in words]\n",
        "    if len(ids) > max_len:\n",
        "        ids = ids[:max_len]\n",
        "    return ids\n",
        "\n",
        "class WordNERDataset(Dataset):\n",
        "    def __init__(self, sents, labels, word2id, max_len=768):\n",
        "        self.samples = []\n",
        "        for words, labs in zip(sents, labels):\n",
        "            if len(words) != len(labs) or len(words) == 0:\n",
        "                continue\n",
        "            if len(words) > max_len:\n",
        "                words, labs = words[:max_len], labs[:max_len]\n",
        "            ids = [word2id.get(w, word2id[UNK_TOKEN]) for w in words]\n",
        "            lab_ids = [LABEL2ID.get(l, 0) for l in labs]\n",
        "            self.samples.append((ids, lab_ids, len(ids)))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        return self.samples[i]\n",
        "\n",
        "def collate_pad(batch):\n",
        "    max_l = max(b[2] for b in batch)\n",
        "    pad_id = word2id[PAD_TOKEN]\n",
        "    pad_label = -100\n",
        "    ids = torch.tensor([b[0] + [pad_id] * (max_l - b[2]) for b in batch], dtype=torch.long)\n",
        "    labels = torch.tensor([b[1] + [pad_label] * (max_l - b[2]) for b in batch], dtype=torch.long)\n",
        "    mask = torch.tensor([[1] * b[2] + [0] * (max_l - b[2]) for b in batch], dtype=torch.long)\n",
        "    return ids, mask, labels\n",
        "\n",
        "MAX_LEN = 768\n",
        "train_ds = WordNERDataset(train_sents, train_labels, word2id, MAX_LEN)\n",
        "val_ds = WordNERDataset(val_sents, val_labels, word2id, MAX_LEN)\n",
        "# Oversample resumes that contain SKILL (and other rare entities) to improve recall\n",
        "rare_tags = {\"B-SKILL\", \"I-SKILL\", \"B-EDUCATION\", \"I-EDUCATION\", \"B-EXPERIENCE\", \"I-EXPERIENCE\", \"B-OCCUPATION\", \"I-OCCUPATION\"}\n",
        "train_weights = [2.5 if any(t in rare_tags for t in l) else 1.0 for w, l in zip(train_sents, train_labels) if len(w) == len(l) and len(w) > 0]\n",
        "from torch.utils.data import WeightedRandomSampler\n",
        "train_sampler = WeightedRandomSampler(weights=train_weights, num_samples=len(train_weights))\n",
        "train_loader = DataLoader(train_ds, batch_size=6, sampler=train_sampler, collate_fn=collate_pad)\n",
        "val_loader = DataLoader(val_ds, batch_size=12, collate_fn=collate_pad)\n",
        "print(\"Datasets ready (MAX_LEN=768 so full SKILLS seen; batch 6/12 for T4)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Model: BiLSTM-CRF (from scratch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "from torchcrf import CRF\n",
        "\n",
        "class BiLSTMCRF(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, hidden_dim, num_labels, embedding_matrix=None, dropout=0.35):\n",
        "        super().__init__()\n",
        "        self.embed = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
        "        if embedding_matrix is not None:\n",
        "            self.embed.weight.data.copy_(torch.from_numpy(embedding_matrix))\n",
        "            self.embed.weight.requires_grad = True\n",
        "        self.lstm = nn.LSTM(embed_dim, hidden_dim // 2, num_layers=2, bidirectional=True, batch_first=True, dropout=0.2)\n",
        "        self.drop = nn.Dropout(dropout)\n",
        "        self.fc = nn.Linear(hidden_dim, num_labels)\n",
        "        self.crf = CRF(num_labels, batch_first=True)\n",
        "\n",
        "    def forward(self, input_ids, mask, labels=None):\n",
        "        x = self.embed(input_ids)\n",
        "        x = self.drop(x)\n",
        "        out, _ = self.lstm(x)\n",
        "        emissions = self.fc(self.drop(out))\n",
        "        mask_b = mask.bool()\n",
        "        if labels is not None:\n",
        "            labels = labels.clone().masked_fill(labels == -100, 0)\n",
        "            return -self.crf(emissions, labels, mask=mask_b, reduction=\"mean\")\n",
        "        return self.crf.decode(emissions, mask=mask_b)\n",
        "\n",
        "HIDDEN_DIM = 384\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "elif hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n",
        "    device = torch.device(\"mps\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "\n",
        "model = BiLSTMCRF(vocab_size, EMBED_DIM, HIDDEN_DIM, NUM_LABELS, embedding_matrix=embedding_matrix, dropout=0.35).to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=5e-5)\n",
        "print(f\"Model on {device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Training (early stopping, validation F1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from seqeval.metrics import f1_score\n",
        "from torch.optim.lr_scheduler import LinearLR, SequentialLR, ConstantLR\n",
        "\n",
        "def run_validation(model, val_loader, device, id2label, num_labels):\n",
        "    model.eval()\n",
        "    true_all, pred_all = [], []\n",
        "    with torch.no_grad():\n",
        "        for ids, mask, labels in val_loader:\n",
        "            ids, mask = ids.to(device), mask.to(device)\n",
        "            preds = model(ids, mask)\n",
        "            for b in range(ids.size(0)):\n",
        "                m, labs = mask[b].cpu(), labels[b].cpu()\n",
        "                pred_b = preds[b]\n",
        "                tlist, plist = [], []\n",
        "                for i in range(m.size(0)):\n",
        "                    if m[i].item() == 0:\n",
        "                        break\n",
        "                    if labs[i].item() == -100:\n",
        "                        continue\n",
        "                    tlist.append(id2label[labs[i].item()])\n",
        "                    p = id2label[pred_b[i]] if i < len(pred_b) and pred_b[i] < num_labels else \"O\"\n",
        "                    plist.append(p)\n",
        "                if tlist and plist:\n",
        "                    true_all.append(tlist)\n",
        "                    pred_all.append(plist)\n",
        "    f1 = f1_score(true_all, pred_all, zero_division=0) if true_all else 0.0\n",
        "    return f1\n",
        "\n",
        "EPOCHS = 120\n",
        "PATIENCE = 25\n",
        "warmup_epochs = max(2, EPOCHS // 15)\n",
        "scheduler = SequentialLR(optimizer, [\n",
        "    ConstantLR(optimizer, factor=0.1, total_iters=warmup_epochs),\n",
        "    LinearLR(optimizer, start_factor=1.0, end_factor=0.15, total_iters=EPOCHS - warmup_epochs),\n",
        "], milestones=[warmup_epochs])\n",
        "best_f1 = 0.0\n",
        "best_state = None\n",
        "epochs_no_improve = 0\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    model.train()\n",
        "    total = 0\n",
        "    for ids, mask, lab in train_loader:\n",
        "        ids, mask, lab = ids.to(device), mask.to(device), lab.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        loss = model(ids, mask, lab)\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 2.0)\n",
        "        optimizer.step()\n",
        "        total += loss.item()\n",
        "    scheduler.step()\n",
        "    val_f1 = run_validation(model, val_loader, device, ID2LABEL, NUM_LABELS)\n",
        "    if val_f1 > best_f1:\n",
        "        best_f1 = val_f1\n",
        "        best_state = {k: v.cpu().clone() for k, v in model.state_dict().items()}\n",
        "        epochs_no_improve = 0\n",
        "    else:\n",
        "        epochs_no_improve += 1\n",
        "    print(f\"Epoch {epoch+1}/{EPOCHS} Loss: {total/len(train_loader):.4f} Val F1: {val_f1:.4f} Best: {best_f1:.4f}\")\n",
        "    if epochs_no_improve >= PATIENCE:\n",
        "        print(f\"Early stopping (no improvement for {PATIENCE} epochs).\")\n",
        "        break\n",
        "if best_state is not None:\n",
        "    model.load_state_dict(best_state)\n",
        "    print(\"Restored best checkpoint (by val F1).\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Save model and assets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Inference on sample text (optional)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def parse_resume_path2(text, word2id, model, device, id2label, max_len=768):\n",
        "    words = re.findall(r\"\\S+\", text)[:max_len]\n",
        "    if not words:\n",
        "        return [], [], {}\n",
        "    ids = torch.tensor([[word2id.get(w, word2id[\"<UNK>\"]) for w in words]], dtype=torch.long).to(device)\n",
        "    mask = torch.ones_like(ids, dtype=torch.long).to(device)\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        preds = model(ids, mask)\n",
        "    pred_tags = [id2label.get(preds[0][i], \"O\") for i in range(len(preds[0]))]\n",
        "    entities = {}\n",
        "    i = 0\n",
        "    while i < len(words):\n",
        "        tag = pred_tags[i] if i < len(pred_tags) else \"O\"\n",
        "        if tag.startswith(\"B-\"):\n",
        "            entity_type = tag[2:]\n",
        "            phrase = [words[i]]\n",
        "            i += 1\n",
        "            while i < len(words) and i < len(pred_tags) and pred_tags[i] == f\"I-{entity_type}\":\n",
        "                phrase.append(words[i])\n",
        "                i += 1\n",
        "            raw = \" \".join(phrase)\n",
        "            cleaned = raw.rstrip(\".,;:\")\n",
        "            entities.setdefault(entity_type, []).append(cleaned)\n",
        "        else:\n",
        "            i += 1\n",
        "    return words, pred_tags, entities\n",
        "\n",
        "sample = \"John Doe john.doe@email.com Software Engineer. Skills: Python Java. BSc University of Colombo 2020. Worked at Tech Corp.\"\n",
        "words, tags, entities = parse_resume_path2(sample, word2id, model, device, ID2LABEL)\n",
        "print(\"Entities:\", entities)\n",
        "print(\"Tags (first 15):\", list(zip(words[:15], tags[:15])))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "_drive_base = \"/content/drive/MyDrive\" if os.path.exists(\"/content/drive/MyDrive\") else \"/content/drive/My Drive\"\n",
        "SAVE_DIR = os.environ.get(\"RESUME_NER_PATH2_SAVE_DIR\", os.path.join(_drive_base, \"resume_ner_bilstm_crf\"))\n",
        "os.makedirs(SAVE_DIR, exist_ok=True)\n",
        "\n",
        "torch.save(model.state_dict(), os.path.join(SAVE_DIR, \"bilstm_crf_state.pt\"))\n",
        "w2v.save(os.path.join(SAVE_DIR, \"word2vec.model\"))\n",
        "config = {\"tags\": TAGS, \"word2id\": word2id, \"embed_dim\": EMBED_DIM, \"num_labels\": NUM_LABELS, \"max_len\": MAX_LEN}\n",
        "with open(os.path.join(SAVE_DIR, \"ner_config.json\"), \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(config, f, indent=2)\n",
        "\n",
        "print(\"Saved:\", SAVE_DIR)\n",
        "print(\"  - bilstm_crf_state.pt\")\n",
        "print(\"  - word2vec.model\")\n",
        "print(\"  - ner_config.json\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Example texts – check extracted values\n",
        "\n",
        "Run the cell below with different resume snippets to see extracted entities."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "EXAMPLE_TEXTS = [\n",
        "    \"Jane Smith jane.smith@gmail.com Data Scientist. Skills: Python, SQL, TensorFlow. MSc University of Moratuwa 2019. Worked at Analytics Ltd.\",\n",
        "    \"Kamal Perera kamal.p@company.lk Senior Software Engineer. Java, Spring, AWS. BSc Computer Science Peradeniya 2014. Experience: Virtusa, WSO2.\",\n",
        "    \"Maria Garcia maria.garcia@outlook.com Product Manager. Agile, Jira. MBA Colombo Business School 2021. Previous: StartupXYZ, Tech Solutions.\"\n",
        "]\n",
        "\n",
        "for i, text in enumerate(EXAMPLE_TEXTS, 1):\n",
        "    words, tags, entities = parse_resume_path2(text, word2id, model, device, ID2LABEL)\n",
        "    print(f\"{'='*60}\\nExample {i}\\n{'='*60}\")\n",
        "    print(\"Extracted entities:\")\n",
        "    for k, v in entities.items():\n",
        "        print(f\"  {k}: {v}\")\n",
        "    print(\"\\nWord-level tags (first 20):\", list(zip(words[:20], tags[:20])))\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Evaluation (validation and test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from seqeval.metrics import classification_report, f1_score\n",
        "\n",
        "model.eval()\n",
        "true_all, pred_all = [], []\n",
        "with torch.no_grad():\n",
        "    for ids, mask, labels in val_loader:\n",
        "        ids, mask = ids.to(device), mask.to(device)\n",
        "        preds = model(ids, mask)\n",
        "        for b in range(ids.size(0)):\n",
        "            m, labs = mask[b].cpu(), labels[b].cpu()\n",
        "            pred_b = preds[b]\n",
        "            tlist, plist = [], []\n",
        "            for i in range(m.size(0)):\n",
        "                if m[i].item() == 0:\n",
        "                    break\n",
        "                if labs[i].item() == -100:\n",
        "                    continue\n",
        "                tlist.append(ID2LABEL[labs[i].item()])\n",
        "                p = ID2LABEL[pred_b[i]] if i < len(pred_b) and pred_b[i] < NUM_LABELS else \"O\"\n",
        "                plist.append(p)\n",
        "            if tlist and plist:\n",
        "                true_all.append(tlist)\n",
        "                pred_all.append(plist)\n",
        "\n",
        "print(\"--- Validation ---\")\n",
        "print(classification_report(true_all, pred_all, zero_division=0))\n",
        "print(\"Val F1:\", f1_score(true_all, pred_all, zero_division=0))\n",
        "\n",
        "test_ds = WordNERDataset(test_sents, test_labels, word2id, MAX_LEN)\n",
        "test_loader = DataLoader(test_ds, batch_size=12, collate_fn=collate_pad)\n",
        "true_test, pred_test = [], []\n",
        "with torch.no_grad():\n",
        "    for ids, mask, labels in test_loader:\n",
        "        ids, mask = ids.to(device), mask.to(device)\n",
        "        preds = model(ids, mask)\n",
        "        for b in range(ids.size(0)):\n",
        "            m, labs = mask[b].cpu(), labels[b].cpu()\n",
        "            pred_b = preds[b]\n",
        "            tlist, plist = [], []\n",
        "            for i in range(m.size(0)):\n",
        "                if m[i].item() == 0:\n",
        "                    break\n",
        "                if labs[i].item() == -100:\n",
        "                    continue\n",
        "                tlist.append(ID2LABEL[labs[i].item()])\n",
        "                p = ID2LABEL[pred_b[i]] if i < len(pred_b) and pred_b[i] < NUM_LABELS else \"O\"\n",
        "                plist.append(p)\n",
        "            if tlist and plist:\n",
        "                true_test.append(tlist)\n",
        "                pred_test.append(plist)\n",
        "\n",
        "print(\"--- Test ---\")\n",
        "print(classification_report(true_test, pred_test, zero_division=0))\n",
        "print(\"Test F1:\", f1_score(true_test, pred_test, zero_division=0))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
