{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Job Poster NER with BiLSTM-CRF from Scratch (Path 2 – FYP)\n",
    "\n",
    "This notebook implements **Path 2** for job poster NER: a **Word2Vec + BiLSTM + CRF** model built from scratch (no pre-trained transformer). Entity types: **JOB_TITLE**, **COMPANY**, **LOCATION**, **SALARY**, **SKILLS_REQUIRED**, **EXPERIENCE_REQUIRED**, **EDUCATION_REQUIRED**, **JOB_TYPE**.\n",
    "\n",
    "- **Feature engineering:** Word embeddings (Word2Vec) trained on job posting corpus\n",
    "- **Model:** Embedding → BiLSTM → Linear → CRF\n",
    "- **Training:** Full training loop with CRF loss\n",
    "- **Data:** `merged_job_poster_ner.json` (e.g. SkillSpan ~11.5k sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependencies\n",
    "\n",
    "Run once. (No `transformers`; uses `gensim` for Word2Vec.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q torch pytorch-crf seqeval gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mount Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from google.colab import drive\n",
    "    drive.mount(\"/content/drive\")\n",
    "    print(\"Drive mounted.\")\n",
    "except ImportError:\n",
    "    print(\"Not in Colab – skip this cell when running locally.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "_drive_base = \"/content/drive/MyDrive\" if os.path.exists(\"/content/drive/MyDrive\") else \"/content/drive/My Drive\"\n",
    "# Prefer merged files with LLM data (SkillSpan + LLM + Sri Lanka jobs)\n",
    "for _name in [\"merged_job_poster_ner_full.json\", \"merged_job_poster_ner_with_llm.json\", \"merged_job_poster_ner.json\"]:\n",
    "    _p = os.path.join(_drive_base, _name)\n",
    "    if os.path.exists(_p):\n",
    "        DATA_PATH = _p\n",
    "        break\n",
    "else:\n",
    "    DATA_PATH = \"../../job_poster_ner_pipeline/merged_job_poster_ner_full.json\"\n",
    "if not os.path.exists(DATA_PATH):\n",
    "    DATA_PATH = \"../../job_poster_ner_pipeline/merged_job_poster_ner_with_llm.json\"\n",
    "if not os.path.exists(DATA_PATH):\n",
    "    DATA_PATH = \"../../job_poster_ner_pipeline/merged_job_poster_ner.json\"\n",
    "if not os.path.exists(DATA_PATH):\n",
    "    raise FileNotFoundError(\"No data file found. Place merged_job_poster_ner_full.json in Drive or job_poster_ner_pipeline/.\")\n",
    "\n",
    "data = []\n",
    "with open(DATA_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "        if line:\n",
    "            data.append(json.loads(line))\n",
    "print(f\"Loaded {len(data)} job postings from {os.path.basename(DATA_PATH)}\")\n",
    "\n",
    "LABEL_MAPPING = {\n",
    "    \"JOB_TITLE\": \"JOB_TITLE\", \"COMPANY\": \"COMPANY\", \"LOCATION\": \"LOCATION\", \"SALARY\": \"SALARY\",\n",
    "    \"SKILLS_REQUIRED\": \"SKILLS_REQUIRED\", \"EXPERIENCE_REQUIRED\": \"EXPERIENCE_REQUIRED\",\n",
    "    \"EDUCATION_REQUIRED\": \"EDUCATION_REQUIRED\", \"JOB_TYPE\": \"JOB_TYPE\", \"O\": \"O\",\n",
    "    \"Job Title\": \"JOB_TITLE\", \"Company\": \"COMPANY\", \"Location\": \"LOCATION\", \"Salary\": \"SALARY\",\n",
    "    \"Skill\": \"SKILLS_REQUIRED\", \"Occupation\": \"JOB_TITLE\", \"Qualification\": \"EDUCATION_REQUIRED\",\n",
    "}\n",
    "for item in data:\n",
    "    for ann in item.get(\"annotation\", []):\n",
    "        ann[\"label\"] = [LABEL_MAPPING.get(l, \"O\") for l in ann[\"label\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Preprocessing and train/val/test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import random\n",
    "\n",
    "def tokenize_with_positions(text):\n",
    "    return [(m.group(), m.start(), m.end()) for m in re.finditer(r\"\\S+\", text)]\n",
    "\n",
    "def create_bio_tags_fixed(tokens, annotations):\n",
    "    bio = [\"O\"] * len(tokens)\n",
    "    for ann in annotations:\n",
    "        if not ann.get(\"label\") or ann[\"label\"][0] == \"O\":\n",
    "            continue\n",
    "        entity = ann[\"label\"][0]\n",
    "        for pt in ann.get(\"points\", []):\n",
    "            s, e = pt[\"start\"], pt[\"end\"]\n",
    "            first = True\n",
    "            for i, (_, ts, te) in enumerate(tokens):\n",
    "                if te <= s or ts >= e:\n",
    "                    continue\n",
    "                bio[i] = f\"B-{entity}\" if first else f\"I-{entity}\"\n",
    "                first = False\n",
    "    return bio\n",
    "\n",
    "all_sents, all_labels = [], []\n",
    "for item in data:\n",
    "    content = item.get(\"content\", \"\")\n",
    "    anns = item.get(\"annotation\", [])\n",
    "    if not content or not anns:\n",
    "        continue\n",
    "    toks = tokenize_with_positions(content)\n",
    "    if not toks:\n",
    "        continue\n",
    "    labs = create_bio_tags_fixed(toks, anns)\n",
    "    all_sents.append([t[0] for t in toks])\n",
    "    all_labels.append(labs)\n",
    "\n",
    "n = len(all_sents)\n",
    "random.seed(42)\n",
    "idx = list(range(n))\n",
    "random.shuffle(idx)\n",
    "n_train, n_val = int(0.8 * n), int(0.1 * n)\n",
    "train_sents = [all_sents[i] for i in idx[:n_train]]\n",
    "train_labels = [all_labels[i] for i in idx[:n_train]]\n",
    "val_sents = [all_sents[i] for i in idx[n_train : n_train + n_val]]\n",
    "val_labels = [all_labels[i] for i in idx[n_train : n_train + n_val]]\n",
    "test_sents = [all_sents[i] for i in idx[n_train + n_val :]]\n",
    "test_labels = [all_labels[i] for i in idx[n_train + n_val :]]\n",
    "print(f\"Train {len(train_sents)} Val {len(val_sents)} Test {len(test_sents)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Word embeddings (Word2Vec) and vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import numpy as np\n",
    "\n",
    "EMBED_DIM = 256\n",
    "W2V_MIN_COUNT = 1\n",
    "W2V_WINDOW = 6\n",
    "W2V_EPOCHS = 35\n",
    "\n",
    "w2v = Word2Vec(sentences=all_sents, vector_size=EMBED_DIM, window=W2V_WINDOW, min_count=W2V_MIN_COUNT, epochs=W2V_EPOCHS, workers=4)\n",
    "\n",
    "PAD_TOKEN = \"<PAD>\"\n",
    "UNK_TOKEN = \"<UNK>\"\n",
    "word2id = {PAD_TOKEN: 0, UNK_TOKEN: 1}\n",
    "for w in w2v.wv.key_to_index:\n",
    "    if w not in word2id:\n",
    "        word2id[w] = len(word2id)\n",
    "vocab_size = len(word2id)\n",
    "\n",
    "embedding_matrix = np.zeros((vocab_size, EMBED_DIM), dtype=np.float32)\n",
    "for w, i in word2id.items():\n",
    "    if w in (PAD_TOKEN, UNK_TOKEN):\n",
    "        continue\n",
    "    if w in w2v.wv:\n",
    "        embedding_matrix[i] = w2v.wv[w]\n",
    "    else:\n",
    "        embedding_matrix[i] = np.random.normal(0, 0.1, EMBED_DIM)\n",
    "\n",
    "print(f\"Vocab size: {vocab_size}, Embed dim: {EMBED_DIM}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Labels and dataset (word-level, padded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "TAGS = [\"O\", \"B-JOB_TITLE\", \"I-JOB_TITLE\", \"B-COMPANY\", \"I-COMPANY\", \"B-LOCATION\", \"I-LOCATION\", \"B-SALARY\", \"I-SALARY\",\n",
    "        \"B-SKILLS_REQUIRED\", \"I-SKILLS_REQUIRED\", \"B-EXPERIENCE_REQUIRED\", \"I-EXPERIENCE_REQUIRED\",\n",
    "        \"B-EDUCATION_REQUIRED\", \"I-EDUCATION_REQUIRED\", \"B-JOB_TYPE\", \"I-JOB_TYPE\"]\n",
    "LABEL2ID = {t: i for i, t in enumerate(TAGS)}\n",
    "ID2LABEL = {i: t for i, t in enumerate(TAGS)}\n",
    "NUM_LABELS = len(TAGS)\n",
    "\n",
    "class WordNERDataset(Dataset):\n",
    "    def __init__(self, sents, labels, word2id, max_len=512):\n",
    "        self.samples = []\n",
    "        for words, labs in zip(sents, labels):\n",
    "            if len(words) != len(labs) or len(words) == 0:\n",
    "                continue\n",
    "            if len(words) > max_len:\n",
    "                words, labs = words[:max_len], labs[:max_len]\n",
    "            ids = [word2id.get(w, word2id[UNK_TOKEN]) for w in words]\n",
    "            lab_ids = [LABEL2ID.get(l, 0) for l in labs]\n",
    "            self.samples.append((ids, lab_ids, len(ids)))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return self.samples[i]\n",
    "\n",
    "def collate_pad(batch):\n",
    "    max_l = max(b[2] for b in batch)\n",
    "    pad_id = word2id[PAD_TOKEN]\n",
    "    pad_label = -100\n",
    "    ids = torch.tensor([b[0] + [pad_id] * (max_l - b[2]) for b in batch], dtype=torch.long)\n",
    "    labels = torch.tensor([b[1] + [pad_label] * (max_l - b[2]) for b in batch], dtype=torch.long)\n",
    "    mask = torch.tensor([[1] * b[2] + [0] * (max_l - b[2]) for b in batch], dtype=torch.long)\n",
    "    return ids, mask, labels\n",
    "\n",
    "MAX_LEN = 512\n",
    "train_ds = WordNERDataset(train_sents, train_labels, word2id, MAX_LEN)\n",
    "val_ds = WordNERDataset(val_sents, val_labels, word2id, MAX_LEN)\n",
    "\n",
    "# Oversample any sample with entities (not just O) to prevent all-O collapse\n",
    "entity_tags = {\"B-SALARY\", \"I-SALARY\", \"B-EDUCATION_REQUIRED\", \"I-EDUCATION_REQUIRED\", \"B-JOB_TYPE\", \"I-JOB_TYPE\",\n",
    "               \"B-EXPERIENCE_REQUIRED\", \"I-EXPERIENCE_REQUIRED\", \"B-COMPANY\", \"I-COMPANY\", \"B-LOCATION\", \"I-LOCATION\",\n",
    "               \"B-JOB_TITLE\", \"I-JOB_TITLE\", \"B-SKILLS_REQUIRED\", \"I-SKILLS_REQUIRED\"}\n",
    "# Higher weight (3.5) for entity-rich samples to prevent model collapsing to all-O predictions\n",
    "train_weights = [3.5 if any(t in entity_tags for t in l) else 1.0 for w, l in zip(train_sents, train_labels) if len(w) == len(l) and len(w) > 0]\n",
    "from torch.utils.data import WeightedRandomSampler\n",
    "train_sampler = WeightedRandomSampler(weights=train_weights, num_samples=len(train_weights))\n",
    "train_loader = DataLoader(train_ds, batch_size=8, sampler=train_sampler, collate_fn=collate_pad)\n",
    "val_loader = DataLoader(val_ds, batch_size=12, collate_fn=collate_pad)\n",
    "print(\"Datasets ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model: BiLSTM-CRF (from scratch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torchcrf import CRF\n",
    "\n",
    "class BiLSTMCRF(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, num_labels, embedding_matrix=None, dropout=0.35):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
    "        if embedding_matrix is not None:\n",
    "            self.embed.weight.data.copy_(torch.from_numpy(embedding_matrix))\n",
    "            self.embed.weight.requires_grad = True\n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_dim // 2, num_layers=2, bidirectional=True, batch_first=True, dropout=0.2)\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(hidden_dim, num_labels)\n",
    "        self.crf = CRF(num_labels, batch_first=True)\n",
    "\n",
    "    def forward(self, input_ids, mask, labels=None):\n",
    "        x = self.embed(input_ids)\n",
    "        x = self.drop(x)\n",
    "        out, _ = self.lstm(x)\n",
    "        emissions = self.fc(self.drop(out))\n",
    "        mask_b = mask.bool()\n",
    "        if labels is not None:\n",
    "            labels = labels.clone().masked_fill(labels == -100, 0)\n",
    "            return -self.crf(emissions, labels, mask=mask_b, reduction=\"mean\")\n",
    "        return self.crf.decode(emissions, mask=mask_b)\n",
    "\n",
    "HIDDEN_DIM = 384\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "elif hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "model = BiLSTMCRF(vocab_size, EMBED_DIM, HIDDEN_DIM, NUM_LABELS, embedding_matrix=embedding_matrix, dropout=0.35).to(device)\n",
    "# Lower LR (5e-4) helps prevent collapse to all-O; raise to 1e-3 if training is too slow\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=5e-4, weight_decay=5e-5)\n",
    "print(f\"Model on {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Training (early stopping, validation F1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from seqeval.metrics import f1_score\n",
    "from torch.optim.lr_scheduler import LinearLR, SequentialLR, ConstantLR\n",
    "\n",
    "def run_validation(model, val_loader, device, id2label, num_labels):\n",
    "    model.eval()\n",
    "    true_all, pred_all = [], []\n",
    "    with torch.no_grad():\n",
    "        for ids, mask, labels in val_loader:\n",
    "            ids, mask = ids.to(device), mask.to(device)\n",
    "            preds = model(ids, mask)\n",
    "            for b in range(ids.size(0)):\n",
    "                m, labs = mask[b].cpu(), labels[b].cpu()\n",
    "                pred_b = preds[b]\n",
    "                tlist, plist = [], []\n",
    "                for i in range(m.size(0)):\n",
    "                    if m[i].item() == 0:\n",
    "                        break\n",
    "                    if labs[i].item() == -100:\n",
    "                        continue\n",
    "                    tlist.append(id2label[labs[i].item()])\n",
    "                    p = id2label[pred_b[i]] if i < len(pred_b) and pred_b[i] < num_labels else \"O\"\n",
    "                    plist.append(p)\n",
    "                if tlist and plist:\n",
    "                    true_all.append(tlist)\n",
    "                    pred_all.append(plist)\n",
    "    f1 = f1_score(true_all, pred_all, zero_division=0) if true_all else 0.0\n",
    "    return f1\n",
    "\n",
    "EPOCHS = 80\n",
    "PATIENCE = 20\n",
    "warmup_epochs = max(2, EPOCHS // 15)\n",
    "scheduler = SequentialLR(optimizer, [\n",
    "    ConstantLR(optimizer, factor=0.1, total_iters=warmup_epochs),\n",
    "    LinearLR(optimizer, start_factor=1.0, end_factor=0.15, total_iters=EPOCHS - warmup_epochs),\n",
    "], milestones=[warmup_epochs])\n",
    "best_f1 = 0.0\n",
    "best_state = None\n",
    "epochs_no_improve = 0\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    total = 0\n",
    "    for ids, mask, lab in train_loader:\n",
    "        ids, mask, lab = ids.to(device), mask.to(device), lab.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        loss = model(ids, mask, lab)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 2.0)\n",
    "        optimizer.step()\n",
    "        total += loss.item()\n",
    "    scheduler.step()\n",
    "    val_f1 = run_validation(model, val_loader, device, ID2LABEL, NUM_LABELS)\n",
    "    if val_f1 > best_f1:\n",
    "        best_f1 = val_f1\n",
    "        best_state = {k: v.cpu().clone() for k, v in model.state_dict().items()}\n",
    "        epochs_no_improve = 0\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS} Loss: {total/len(train_loader):.4f} Val F1: {val_f1:.4f} Best: {best_f1:.4f}\")\n",
    "    if epochs_no_improve >= PATIENCE:\n",
    "        print(f\"Early stopping (no improvement for {PATIENCE} epochs).\")\n",
    "        break\n",
    "if best_state is not None:\n",
    "    model.load_state_dict(best_state)\n",
    "    print(\"Restored best checkpoint (by val F1).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Save model and assets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_save_base = \"/content/drive/MyDrive\" if os.path.exists(\"/content/drive/MyDrive\") else \".\"\n",
    "SAVE_DIR = os.environ.get(\"JOB_POSTER_NER_PATH2_SAVE_DIR\", os.path.join(_save_base, \"job_poster_ner_path2\"))\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "\n",
    "torch.save(model.state_dict(), os.path.join(SAVE_DIR, \"bilstm_crf_state.pt\"))\n",
    "w2v.save(os.path.join(SAVE_DIR, \"word2vec.model\"))\n",
    "config = {\"tags\": TAGS, \"word2id\": word2id, \"embed_dim\": EMBED_DIM, \"num_labels\": NUM_LABELS, \"max_len\": MAX_LEN}\n",
    "with open(os.path.join(SAVE_DIR, \"ner_config.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(config, f, indent=2)\n",
    "\n",
    "print(\"Saved:\", SAVE_DIR)\n",
    "print(\"  - bilstm_crf_state.pt\")\n",
    "print(\"  - word2vec.model\")\n",
    "print(\"  - ner_config.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Inference on job poster text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_job_poster_path2(text, word2id, model, device, id2label, max_len=512):\n",
    "    \"\"\"Extract entities from job poster text using Path 2 model.\"\"\"\n",
    "    words = re.findall(r\"\\S+\", text)[:max_len]\n",
    "    if not words:\n",
    "        return [], [], {}\n",
    "    ids = torch.tensor([[word2id.get(w, word2id[UNK_TOKEN]) for w in words]], dtype=torch.long).to(device)\n",
    "    mask = torch.ones_like(ids, dtype=torch.long).to(device)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        preds = model(ids, mask)\n",
    "    pred_tags = [id2label.get(preds[0][i], \"O\") for i in range(len(preds[0]))]\n",
    "    entities = {}\n",
    "    i = 0\n",
    "    while i < len(words):\n",
    "        tag = pred_tags[i] if i < len(pred_tags) else \"O\"\n",
    "        if tag.startswith(\"B-\"):\n",
    "            entity_type = tag[2:]\n",
    "            phrase = [words[i]]\n",
    "            i += 1\n",
    "            while i < len(words) and i < len(pred_tags) and pred_tags[i] == f\"I-{entity_type}\":\n",
    "                phrase.append(words[i])\n",
    "                i += 1\n",
    "            raw = \" \".join(phrase)\n",
    "            cleaned = raw.strip().rstrip(\".,;:!?)]}\\\"'\").lstrip(\"([{\\\"'\").strip()\n",
    "            if cleaned:\n",
    "                entities.setdefault(entity_type, []).append(cleaned)\n",
    "        else:\n",
    "            i += 1\n",
    "    for k in entities:\n",
    "        entities[k] = list(dict.fromkeys(entities[k]))\n",
    "    return words, pred_tags, entities\n",
    "\n",
    "# Simpler regex to avoid escaping issues (catches $80k, $120k-150k, 80k-120k, Competitive)\n",
    "SALARY_RE = re.compile(r'\\\\$[\\\\d,]+\\\\.?\\\\d*\\\\s*(k|K|M)?|\\\\d+\\\\s*(k|K)\\\\s*-\\\\s*\\\\d+\\\\s*(k|K)|Competitive', re.IGNORECASE)\n",
    "\n",
    "def parse_job_poster_path2_hybrid(text, word2id, model, device, id2label, max_len=512):\n",
    "    \"\"\"Hybrid: SALARY from rules, rest from model.\"\"\"\n",
    "    words, pred_tags, entities = parse_job_poster_path2(text, word2id, model, device, id2label, max_len)\n",
    "    sal = list(dict.fromkeys(m.group(0).strip() for m in SALARY_RE.finditer(text)))\n",
    "    if sal:\n",
    "        entities[\"SALARY\"] = sal\n",
    "    return words, pred_tags, entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "JOB_POSTER_TEXT = \"\"\"\n",
    "Senior Data Scientist at Acme Corp. Remote. $120k-150k. Skills: Python, ML. 5+ years experience. PhD preferred. Full-time.\n",
    "\"\"\"\n",
    "words, tags, entities = parse_job_poster_path2_hybrid(JOB_POSTER_TEXT.strip(), word2id, model, device, ID2LABEL)\n",
    "print(\"Entities (hybrid):\")\n",
    "for k, v in entities.items():\n",
    "    print(f\"  {k}: {v}\")\n",
    "print(\"\\nWord-level tags (first 25):\", list(zip(words[:25], tags[:25])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Evaluation (validation and test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from seqeval.metrics import classification_report, f1_score\n",
    "\n",
    "model.eval()\n",
    "true_all, pred_all = [], []\n",
    "with torch.no_grad():\n",
    "    for ids, mask, labels in val_loader:\n",
    "        ids, mask = ids.to(device), mask.to(device)\n",
    "        preds = model(ids, mask)\n",
    "        for b in range(ids.size(0)):\n",
    "            m, labs = mask[b].cpu(), labels[b].cpu()\n",
    "            pred_b = preds[b]\n",
    "            tlist, plist = [], []\n",
    "            for i in range(m.size(0)):\n",
    "                if m[i].item() == 0:\n",
    "                    break\n",
    "                if labs[i].item() == -100:\n",
    "                    continue\n",
    "                tlist.append(ID2LABEL[labs[i].item()])\n",
    "                p = ID2LABEL[pred_b[i]] if i < len(pred_b) and pred_b[i] < NUM_LABELS else \"O\"\n",
    "                plist.append(p)\n",
    "            if tlist and plist:\n",
    "                true_all.append(tlist)\n",
    "                pred_all.append(plist)\n",
    "\n",
    "print(\"--- Validation ---\")\n",
    "print(classification_report(true_all, pred_all, zero_division=0))\n",
    "print(\"Val F1:\", f1_score(true_all, pred_all, zero_division=0))\n",
    "\n",
    "test_ds = WordNERDataset(test_sents, test_labels, word2id, MAX_LEN)\n",
    "test_loader = DataLoader(test_ds, batch_size=12, collate_fn=collate_pad)\n",
    "true_test, pred_test = [], []\n",
    "with torch.no_grad():\n",
    "    for ids, mask, labels in test_loader:\n",
    "        ids, mask = ids.to(device), mask.to(device)\n",
    "        preds = model(ids, mask)\n",
    "        for b in range(ids.size(0)):\n",
    "            m, labs = mask[b].cpu(), labels[b].cpu()\n",
    "            pred_b = preds[b]\n",
    "            tlist, plist = [], []\n",
    "            for i in range(m.size(0)):\n",
    "                if m[i].item() == 0:\n",
    "                    break\n",
    "                if labs[i].item() == -100:\n",
    "                    continue\n",
    "                tlist.append(ID2LABEL[labs[i].item()])\n",
    "                p = ID2LABEL[pred_b[i]] if i < len(pred_b) and pred_b[i] < NUM_LABELS else \"O\"\n",
    "                plist.append(p)\n",
    "            if tlist and plist:\n",
    "                true_test.append(tlist)\n",
    "                pred_test.append(plist)\n",
    "\n",
    "print(\"--- Test ---\")\n",
    "print(classification_report(true_test, pred_test, zero_division=0))\n",
    "print(\"Test F1:\", f1_score(true_test, pred_test, zero_division=0))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
