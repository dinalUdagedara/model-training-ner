{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# BERT-BiLSTM-CRF for Resume NER (merged pipeline)\n",
        "\n",
        "Uses merged data (existing 220 + Dotin 545). Run prepare_data.py first to create merged_resume_ner.json.\n",
        "\n",
        "**Target:** 80%+ entity-level F1 (or 80%+ token accuracy). Training uses early stopping, differentiated LRs (BERT 1e-5, head 5e-5), LR warmup + decay, and dropout 0.4.\n",
        "\n",
        "Run cells in order. JSON file is loaded from the same folder as this notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded 580 resumes\n"
          ]
        }
      ],
      "source": [
        "# 1) Load data — prefers merged_resume_ner.json (run prepare_data.py first)\n",
        "import json\n",
        "import os\n",
        "\n",
        "DATA_PATH = \"merged_resume_ner.json\"\n",
        "if not os.path.exists(DATA_PATH):\n",
        "    DATA_PATH = \"entity_recognition_in_resumes.json\"\n",
        "if not os.path.exists(DATA_PATH):\n",
        "    DATA_PATH = \"/content/drive/My Drive/DATASETS/merged_resume_ner.json\"\n",
        "if not os.path.exists(DATA_PATH):\n",
        "    raise FileNotFoundError(\"JSON not found. Run: python prepare_data.py --existing ../entity_recognition_in_resumes.json --dotin /path/to/dotin --output merged_resume_ner.json\")\n",
        "\n",
        "data = []\n",
        "with open(DATA_PATH, \"r\", encoding=\"utf-8\") as f:\n",
        "    for line in f:\n",
        "        line = line.strip()\n",
        "        if line:\n",
        "            data.append(json.loads(line))\n",
        "print(f\"Loaded {len(data)} resumes\")\n",
        "\n",
        "LABEL_MAPPING = {\n",
        "    \"Name\": \"NAME\", \"Email Address\": \"EMAIL\", \"Skills\": \"SKILL\", \"Designation\": \"OCCUPATION\",\n",
        "    \"Degree\": \"EDUCATION\", \"College Name\": \"EDUCATION\", \"Graduation Year\": \"EDUCATION\",\n",
        "    \"Companies worked at\": \"EXPERIENCE\", \"Years of Experience\": \"EXPERIENCE\", \"Location\": \"O\", \"UNKNOWN\": \"O\",\n",
        "    \"NAME\": \"NAME\", \"EMAIL\": \"EMAIL\", \"SKILL\": \"SKILL\", \"OCCUPATION\": \"OCCUPATION\", \"EDUCATION\": \"EDUCATION\", \"EXPERIENCE\": \"EXPERIENCE\", \"O\": \"O\",\n",
        "}\n",
        "for item in data:\n",
        "    for ann in item.get(\"annotation\", []):\n",
        "        ann[\"label\"] = [LABEL_MAPPING.get(l, \"O\") for l in ann[\"label\"]]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Colab only – use if upload widget didn't work:** Run the cell below to mount Google Drive. Put `entity_recognition_in_resumes.json` in your Drive (e.g. in My Drive), then in **cell 1** set `DATA_PATH = '/content/drive/MyDrive/entity_recognition_in_resumes.json'` (or the path where you put it) and re-run cell 1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Not in Colab – skip this cell.\n"
          ]
        }
      ],
      "source": [
        "# Colab: Mount Google Drive (run this, then set DATA_PATH in cell 1 to your file path in Drive)\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount(\"/content/drive\")\n",
        "    print(\"Drive mounted. Put entity_recognition_in_resumes.json in My Drive, then set DATA_PATH in cell 1 and re-run it.\")\n",
        "except ImportError:\n",
        "    print(\"Not in Colab – skip this cell.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train 464 Val 58 Test 58\n"
          ]
        }
      ],
      "source": [
        "# 2) Build train/val/test from JSON with fixed create_bio_tags (no B-O / I-O)\n",
        "import re\n",
        "import random\n",
        "\n",
        "def tokenize_with_positions(text):\n",
        "    return [(m.group(), m.start(), m.end()) for m in re.finditer(r\"\\S+\", text)]\n",
        "\n",
        "def create_bio_tags_fixed(tokens, annotations):\n",
        "    bio = [\"O\"] * len(tokens)\n",
        "    for ann in annotations:\n",
        "        if not ann.get(\"label\") or ann[\"label\"][0] == \"O\":\n",
        "            continue\n",
        "        entity = ann[\"label\"][0]\n",
        "        for pt in ann.get(\"points\", []):\n",
        "            s, e = pt[\"start\"], pt[\"end\"]\n",
        "            first = True\n",
        "            for i, (_, ts, te) in enumerate(tokens):\n",
        "                if te <= s or ts >= e: continue\n",
        "                bio[i] = f\"B-{entity}\" if first else f\"I-{entity}\"\n",
        "                first = False\n",
        "    return bio\n",
        "\n",
        "all_sents, all_labels = [], []\n",
        "for item in data:\n",
        "    content = item.get(\"content\", \"\")\n",
        "    anns = item.get(\"annotation\", [])\n",
        "    if not content or not anns: continue\n",
        "    toks = tokenize_with_positions(content)\n",
        "    if not toks: continue\n",
        "    labs = create_bio_tags_fixed(toks, anns)\n",
        "    all_sents.append([t[0] for t in toks])\n",
        "    all_labels.append(labs)\n",
        "\n",
        "n = len(all_sents)\n",
        "random.seed(42)\n",
        "idx = list(range(n)); random.shuffle(idx)\n",
        "n_train, n_val = int(0.8 * n), int(0.1 * n)\n",
        "train_sents = [all_sents[i] for i in idx[:n_train]]\n",
        "train_labels = [all_labels[i] for i in idx[:n_train]]\n",
        "val_sents   = [all_sents[i] for i in idx[n_train:n_train+n_val]]\n",
        "val_labels  = [all_labels[i] for i in idx[n_train:n_train+n_val]]\n",
        "test_sents  = [all_sents[i] for i in idx[n_train+n_val:]]\n",
        "test_labels = [all_labels[i] for i in idx[n_train+n_val:]]\n",
        "print(f\"Train {len(train_sents)} Val {len(val_sents)} Test {len(test_sents)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 3) Install deps (run once; skip if already installed)\n",
        "!pip install -q torch transformers pytorch-crf seqeval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "c:\\Users\\HP\\Desktop\\IIT\\4th Year\\model-training-ner\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Datasets ready\n"
          ]
        }
      ],
      "source": [
        "# 4) BERT tokenizer + label alignment and dataset\n",
        "from transformers import BertTokenizer\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch\n",
        "\n",
        "TAGS = [\"O\",\"B-NAME\",\"I-NAME\",\"B-EMAIL\",\"I-EMAIL\",\"B-SKILL\",\"I-SKILL\",\"B-OCCUPATION\",\"I-OCCUPATION\",\"B-EXPERIENCE\",\"I-EXPERIENCE\",\"B-EDUCATION\",\"I-EDUCATION\"]\n",
        "LABEL2ID = {t:i for i,t in enumerate(TAGS)}\n",
        "ID2LABEL = {i:t for i,t in enumerate(TAGS)}\n",
        "NUM_LABELS = len(TAGS)\n",
        "\n",
        "def align_to_bert(words, word_labels, tokenizer, max_len=512):\n",
        "    first_idx, toks = [], [\"[CLS]\"]\n",
        "    for w in words:\n",
        "        p = tokenizer.tokenize(w) or [tokenizer.unk_token]\n",
        "        first_idx.append(len(toks))\n",
        "        toks.extend(p)\n",
        "    toks.append(\"[SEP]\")\n",
        "    ids = tokenizer.convert_tokens_to_ids(toks)\n",
        "    mask = [1]*len(ids)\n",
        "    aligned = [-100]*len(ids)\n",
        "    for pos, lab in zip(first_idx, word_labels):\n",
        "        if pos < len(aligned): aligned[pos] = LABEL2ID.get(lab, 0)\n",
        "    if len(ids) > max_len:\n",
        "        ids, mask, aligned = ids[:max_len-1]+[tokenizer.sep_token_id], mask[:max_len-1]+[1], aligned[:max_len-1]+[-100]\n",
        "    return ids, mask, aligned\n",
        "\n",
        "class BertNERDataset(Dataset):\n",
        "    def __init__(self, sents, labels, tokenizer, max_len=512):\n",
        "        self.samples = [align_to_bert(w, l, tokenizer, max_len) for w, l in zip(sents, labels) if len(w)==len(l)]\n",
        "    def __len__(self): return len(self.samples)\n",
        "    def __getitem__(self, i): return self.samples[i]\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "train_ds = BertNERDataset(train_sents, train_labels, tokenizer)\n",
        "val_ds   = BertNERDataset(val_sents, val_labels, tokenizer)\n",
        "\n",
        "def collate(batch):\n",
        "    max_l = max(len(b[0]) for b in batch)\n",
        "    pad = 0\n",
        "    return (\n",
        "        torch.tensor([b[0]+[pad]*(max_l-len(b[0])) for b in batch], dtype=torch.long),\n",
        "        torch.tensor([b[1]+[0]*(max_l-len(b[1])) for b in batch], dtype=torch.long),\n",
        "        torch.tensor([b[2]+[-100]*(max_l-len(b[2])) for b in batch], dtype=torch.long),\n",
        "    )\n",
        "train_loader = DataLoader(train_ds, batch_size=8, shuffle=True, collate_fn=collate)\n",
        "val_loader   = DataLoader(val_ds, batch_size=8, collate_fn=collate)\n",
        "print(\"Datasets ready\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading weights: 100%|██████████| 199/199 [00:00<00:00, 546.85it/s, Materializing param=pooler.dense.weight]                               \n",
            "BertModel LOAD REPORT from: bert-base-uncased\n",
            "Key                                        | Status     |  | \n",
            "-------------------------------------------+------------+--+-\n",
            "cls.seq_relationship.bias                  | UNEXPECTED |  | \n",
            "cls.predictions.bias                       | UNEXPECTED |  | \n",
            "cls.predictions.transform.dense.weight     | UNEXPECTED |  | \n",
            "cls.predictions.transform.dense.bias       | UNEXPECTED |  | \n",
            "cls.seq_relationship.weight                | UNEXPECTED |  | \n",
            "cls.predictions.transform.LayerNorm.bias   | UNEXPECTED |  | \n",
            "cls.predictions.transform.LayerNorm.weight | UNEXPECTED |  | \n",
            "\n",
            "Notes:\n",
            "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model on cpu\n"
          ]
        }
      ],
      "source": [
        "# 5) BERT-BiLSTM-CRF model\n",
        "import torch.nn as nn\n",
        "from transformers import BertModel\n",
        "from torchcrf import CRF\n",
        "\n",
        "class BertBiLSTMCRF(nn.Module):\n",
        "    def __init__(self, bert_name=\"bert-base-uncased\", hidden_dim=256, num_labels=NUM_LABELS, dropout=0.3):\n",
        "        super().__init__()\n",
        "        self.bert = BertModel.from_pretrained(bert_name)\n",
        "        self.lstm = nn.LSTM(self.bert.config.hidden_size, hidden_dim//2, num_layers=1, bidirectional=True, batch_first=True)\n",
        "        self.drop = nn.Dropout(dropout)\n",
        "        self.fc = nn.Linear(hidden_dim, num_labels)\n",
        "        self.crf = CRF(num_labels, batch_first=True)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, labels=None):\n",
        "        out = self.bert(input_ids=input_ids, attention_mask=attention_mask).last_hidden_state\n",
        "        out, _ = self.lstm(self.drop(out))\n",
        "        emissions = self.fc(self.drop(out))\n",
        "        mask_b = attention_mask.bool()  # boolean avoids torch.where uint8 deprecation in pytorch-crf\n",
        "        if labels is not None:\n",
        "            # CRF expects indices in [0, num_labels-1]; replace padding -100 with 0 (mask ignores those)\n",
        "            labels = labels.clone().masked_fill(labels == -100, 0)\n",
        "            return -self.crf(emissions, labels, mask=mask_b, reduction=\"mean\")\n",
        "        return self.crf.decode(emissions, mask=mask_b)\n",
        "\n",
        "# Prefer GPU (CUDA or Apple Silicon MPS), else CPU\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "elif hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n",
        "    device = torch.device(\"mps\")  # Apple Silicon GPU\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "model = BertBiLSTMCRF(dropout=0.4).to(device)  # Slightly higher dropout for regularization\n",
        "\n",
        "# Differentiated LRs: BERT lower (fine-tune gently), task head higher (learn faster)\n",
        "no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
        "bert_params = list(model.bert.named_parameters())\n",
        "optimizer_grouped = [\n",
        "    {\"params\": [p for n, p in bert_params if not any(nd in n for nd in no_decay)], \"lr\": 1e-5, \"weight_decay\": 0.01},\n",
        "    {\"params\": [p for n, p in bert_params if any(nd in n for nd in no_decay)], \"lr\": 1e-5, \"weight_decay\": 0.0},\n",
        "    {\"params\": [p for n, p in model.named_parameters() if not n.startswith(\"bert.\") and not any(nd in n for nd in no_decay)], \"lr\": 5e-5, \"weight_decay\": 0.01},\n",
        "    {\"params\": [p for n, p in model.named_parameters() if not n.startswith(\"bert.\") and any(nd in n for nd in no_decay)], \"lr\": 5e-5, \"weight_decay\": 0.0},\n",
        "]\n",
        "optimizer = torch.optim.AdamW(optimizer_grouped)\n",
        "print(f\"Model on {device}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/15 Loss: 403.4545\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      8\u001b[39m optimizer.zero_grad()\n\u001b[32m      9\u001b[39m loss = model(inp, mask, lab)\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     11\u001b[39m torch.nn.utils.clip_grad_norm_(model.parameters(), \u001b[32m1.0\u001b[39m)\n\u001b[32m     12\u001b[39m optimizer.step()\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\HP\\Desktop\\IIT\\4th Year\\model-training-ner\\.venv\\Lib\\site-packages\\torch\\_tensor.py:630\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    620\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    621\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    622\u001b[39m         Tensor.backward,\n\u001b[32m    623\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    628\u001b[39m         inputs=inputs,\n\u001b[32m    629\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m630\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    631\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    632\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\HP\\Desktop\\IIT\\4th Year\\model-training-ner\\.venv\\Lib\\site-packages\\torch\\autograd\\__init__.py:364\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    359\u001b[39m     retain_graph = create_graph\n\u001b[32m    361\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    362\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    363\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m364\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    365\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    366\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    367\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    368\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    369\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_tuple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    370\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    371\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    372\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\HP\\Desktop\\IIT\\4th Year\\model-training-ner\\.venv\\Lib\\site-packages\\torch\\autograd\\graph.py:865\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    863\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    864\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m865\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    866\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    867\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    868\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    869\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
            "\u001b[31mKeyboardInterrupt\u001b[39m: "
          ]
        }
      ],
      "source": [
        "# 6) Train with early stopping + val F1 (aim for 80%+ F1)\n",
        "from seqeval.metrics import f1_score\n",
        "\n",
        "def run_validation(model, val_loader, device, id2label, num_labels):\n",
        "    \"\"\"Return (val_f1, true_all, pred_all) for early stopping and reporting.\"\"\"\n",
        "    model.eval()\n",
        "    true_all, pred_all = [], []\n",
        "    with torch.no_grad():\n",
        "        for inp, mask, labels in val_loader:\n",
        "            inp, mask = inp.to(device), mask.to(device)\n",
        "            preds = model(inp, mask)\n",
        "            for b in range(inp.size(0)):\n",
        "                m, labs = mask[b].cpu(), labels[b].cpu()\n",
        "                pred_b = preds[b]\n",
        "                tlist, plist = [], []\n",
        "                pos = 0\n",
        "                for i in range(m.size(0)):\n",
        "                    if m[i].item() == 0:\n",
        "                        break\n",
        "                    p = id2label[pred_b[pos]] if pos < len(pred_b) and pred_b[pos] < num_labels else \"O\"\n",
        "                    pos += 1\n",
        "                    if labs[i].item() == -100:\n",
        "                        continue\n",
        "                    tlist.append(id2label[labs[i].item()])\n",
        "                    plist.append(p)\n",
        "                if tlist and plist:\n",
        "                    true_all.append(tlist)\n",
        "                    pred_all.append(plist)\n",
        "    f1 = f1_score(true_all, pred_all, zero_division=0) if true_all else 0.0\n",
        "    return f1, true_all, pred_all\n",
        "\n",
        "EPOCHS = 30\n",
        "PATIENCE = 5  # stop if val F1 doesn't improve for this many epochs\n",
        "best_f1 = 0.0\n",
        "best_state = None\n",
        "epochs_no_improve = 0\n",
        "\n",
        "# LR scheduler: warmup 10% then linear decay (optional; use transformers for warmup)\n",
        "from torch.optim.lr_scheduler import LinearLR, SequentialLR, ConstantLR\n",
        "warmup_epochs = max(1, EPOCHS // 10)\n",
        "scheduler = SequentialLR(optimizer, [\n",
        "    ConstantLR(optimizer, factor=0.1, total_iters=warmup_epochs),\n",
        "    LinearLR(optimizer, start_factor=1.0, end_factor=0.2, total_iters=EPOCHS - warmup_epochs),\n",
        "], milestones=[warmup_epochs])\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    model.train()\n",
        "    total = 0\n",
        "    for inp, mask, lab in train_loader:\n",
        "        inp, mask, lab = inp.to(device), mask.to(device), lab.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        loss = model(inp, mask, lab)\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        optimizer.step()\n",
        "        total += loss.item()\n",
        "    print(f\"Epoch {epoch+1}/{EPOCHS} Loss: {total/len(train_loader):.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "### 8) Save model\n",
        "Run after training and evaluation. Saves to current folder; in Colab set `SAVE_DIR` to a path in your Drive if you mounted it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save model, tokenizer, and label config (run after training + evaluation)\n",
        "import json\n",
        "import os\n",
        "\n",
        "# In Colab with Drive mounted, use e.g. \"/content/drive/MyDrive/resume_ner\"\n",
        "# Otherwise saves in current directory\n",
        "SAVE_DIR = os.environ.get(\"RESUME_NER_SAVE_DIR\", \".\")\n",
        "os.makedirs(SAVE_DIR, exist_ok=True)\n",
        "\n",
        "# 1) Model weights\n",
        "torch.save(model.state_dict(), os.path.join(SAVE_DIR, \"bert_bilstm_crf_state.pt\"))\n",
        "\n",
        "# 2) Config (tags + bert name for loading later)\n",
        "config = {\"tags\": TAGS, \"bert_name\": \"bert-base-uncased\", \"num_labels\": NUM_LABELS}\n",
        "with open(os.path.join(SAVE_DIR, \"ner_config.json\"), \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(config, f, indent=2)\n",
        "\n",
        "# 3) Tokenizer (so you can load without re-downloading BERT vocab)\n",
        "tokenizer.save_pretrained(SAVE_DIR)\n",
        "\n",
        "print(\"Saved:\", SAVE_DIR)\n",
        "print(\"  - bert_bilstm_crf_state.pt\")\n",
        "print(\"  - ner_config.json\")\n",
        "print(\"  - tokenizer files (vocab.txt, config.json from tokenizer)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 9) Run summary (for your report)\n",
        "Run after cell 7 (Evaluate). Prints a short summary you can copy into your FYP write-up."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run summary (computes val metrics if true_all/pred_all not already from Evaluate cell)\n",
        "from seqeval.metrics import f1_score, precision_score, recall_score, classification_report\n",
        "\n",
        "try:\n",
        "    _ = true_all\n",
        "    _ = pred_all\n",
        "except NameError:\n",
        "    model.eval()\n",
        "    true_all, pred_all = [], []\n",
        "    with torch.no_grad():\n",
        "        for inp, mask, labels in val_loader:\n",
        "            inp, mask = inp.to(device), mask.to(device)\n",
        "            preds = model(inp, mask)\n",
        "            for b in range(inp.size(0)):\n",
        "                m, labs = mask[b].cpu(), labels[b].cpu()\n",
        "                pred_b = preds[b]\n",
        "                tlist, plist = [], []\n",
        "                pos = 0\n",
        "                for i in range(m.size(0)):\n",
        "                    if m[i].item()==0: break\n",
        "                    p = ID2LABEL[pred_b[pos]] if pos < len(pred_b) and pred_b[pos] < NUM_LABELS else \"O\"\n",
        "                    pos += 1\n",
        "                    if labs[i].item() == -100: continue\n",
        "                    tlist.append(ID2LABEL[labs[i].item()])\n",
        "                    plist.append(p)\n",
        "                if tlist and plist:\n",
        "                    true_all.append(tlist)\n",
        "                    pred_all.append(plist)\n",
        "\n",
        "f1 = f1_score(true_all, pred_all, zero_division=0)\n",
        "prec = precision_score(true_all, pred_all, zero_division=0)\n",
        "rec = recall_score(true_all, pred_all, zero_division=0)\n",
        "\n",
        "print(\"--- Run summary (copy for report) ---\")\n",
        "print(f\"Epochs: {EPOCHS}  |  Train size: {len(train_sents)}  |  Val size: {len(val_sents)}\")\n",
        "print(f\"F1 (entity-level): {f1:.4f}\")\n",
        "print(f\"Precision: {prec:.4f}  |  Recall: {rec:.4f}\")\n",
        "print(\"---\")\n",
        "print(classification_report(true_all, pred_all, zero_division=0))\n",
        "print(\"--- End summary ---\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 10) Load saved model (optional)\n",
        "Use when you want to load a previously saved model without re-training. Run cells 1–5 first (data + tokenizer + device), then this cell. You can skip training (cell 6) and go straight to evaluation (cell 7)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load from a saved directory (set LOAD_DIR to where you saved, e.g. \".\" or \"/content/drive/MyDrive/resume_ner\")\n",
        "LOAD_DIR = os.environ.get(\"RESUME_NER_LOAD_DIR\", \".\")\n",
        "with open(os.path.join(LOAD_DIR, \"ner_config.json\"), \"r\", encoding=\"utf-8\") as f:\n",
        "    load_config = json.load(f)\n",
        "\n",
        "# Rebuild label mappings\n",
        "TAGS = load_config[\"tags\"]\n",
        "LABEL2ID = {t: i for i, t in enumerate(TAGS)}\n",
        "ID2LABEL = {i: t for i, t in enumerate(TAGS)}\n",
        "NUM_LABELS = load_config[\"num_labels\"]\n",
        "\n",
        "# Recreate model and load weights\n",
        "from transformers import BertTokenizer, BertModel\n",
        "tokenizer = BertTokenizer.from_pretrained(LOAD_DIR)\n",
        "model = BertBiLSTMCRF(bert_name=load_config[\"bert_name\"], num_labels=NUM_LABELS).to(device)\n",
        "model.load_state_dict(torch.load(os.path.join(LOAD_DIR, \"bert_bilstm_crf_state.pt\"), map_location=device))\n",
        "model.eval()\n",
        "\n",
        "print(\"Model loaded from\", LOAD_DIR)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Hybrid: NAME/EMAIL from rules (high recall), SKILL/EXPERIENCE/EDUCATION/OCCUPATION from model. Defined in cell below."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Parse a new resume (inference, hybrid)\n",
        "Run after training (or after loading a saved model). Put resume text in `RESUME_TEXT` and run. NAME and EMAIL use rule-based extraction; SKILL, EXPERIENCE, EDUCATION, OCCUPATION use the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test with another sample resume (run after the cell above; uses same parse_resume_hybrid)\n",
        "TEST_RESUME_TEXT = \"\"\"\n",
        "Sarah Chen\n",
        "sarah.chen@example.com\n",
        "(555) 123-4567\n",
        "\n",
        "Senior Data Scientist with 8 years of experience in machine learning and NLP.\n",
        "\n",
        "Skills: Python, TensorFlow, PyTorch, SQL, AWS, Spark, Natural Language Processing.\n",
        "\n",
        "Education: PhD in Computer Science, MIT 2016. MSc Statistics, Stanford University 2012. BSc Mathematics, University of California Berkeley 2010.\n",
        "\n",
        "Experience: Lead Data Scientist at Google 2019 Present. Data Scientist at Amazon 2016 2019. Research Intern at Microsoft 2015.\n",
        "\n",
        "Worked at Meta and Netflix. Years of Experience: 8.\n",
        "\"\"\"\n",
        "\n",
        "words2, tags2, entities2 = parse_resume_hybrid(TEST_RESUME_TEXT.strip(), tokenizer, model, device, ID2LABEL)\n",
        "print(\"Entities extracted (hybrid):\")\n",
        "for k, v in entities2.items():\n",
        "    print(f\"  {k}: {v}\")\n",
        "print(\"\\nWord-level tags (first 40):\", list(zip(words2[:40], tags2[:40])))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Unstructured resume (real-CV style: messy formatting, abbreviations, mixed sections)\n",
        "UNSTRUCTURED_RESUME_TEXT = \"\"\"\n",
        "RAJ PATEL   raj.patel@gmail.com   07123456789   London UK\n",
        "\n",
        "PROFILE\n",
        "Full-stack dev, 6+ yrs exp. Built APIs & web apps. Quick learner.\n",
        "\n",
        "SKILLS  Python  Django  React  Node.js  PostgreSQL  Docker  AWS  Git  REST APIs\n",
        "\n",
        "EDUCATION\n",
        "BSc CS Univ of Birmingham 2018   A-Levels King Edward VI 2015\n",
        "\n",
        "WORK\n",
        "• 2021–now  Senior Developer @ FinTech Solutions Ltd  – led 2 devs, shipped payment API\n",
        "• 2019–21   Developer  Acme Corp  – React frontends, bug fixes, 3 yrs of experience there\n",
        "• 2018–19   Junior dev  StartupXYZ  – PHP, MySQL, learned agile\n",
        "\n",
        "Other:  Freelance 2017–18.  References on request.  LinkedIn: linkedin.com/in/rajpatel\n",
        "\"\"\"\n",
        "\n",
        "words_u, tags_u, entities_u = parse_resume_hybrid(UNSTRUCTURED_RESUME_TEXT.strip(), tokenizer, model, device, ID2LABEL)\n",
        "print(\"Entities extracted (unstructured CV):\")\n",
        "for k, v in entities_u.items():\n",
        "    print(f\"  {k}: {v}\")\n",
        "print(\"\\nWord-level tags (first 50):\", list(zip(words_u[:50], tags_u[:50])))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "def parse_resume(text, tokenizer, model, device, id2label, max_len=512):\n",
        "    \"\"\"Tokenize resume text, run NER, return (words, tags) and entity dict.\"\"\"\n",
        "    words = re.findall(r\"\\S+\", text)\n",
        "    if not words:\n",
        "        return [], [], {}\n",
        "    first_idx, toks = [], [\"[CLS]\"]\n",
        "    for w in words:\n",
        "        sub = tokenizer.tokenize(w) or [tokenizer.unk_token]\n",
        "        first_idx.append(len(toks))\n",
        "        toks.extend(sub)\n",
        "    toks.append(\"[SEP]\")\n",
        "    ids = tokenizer.convert_tokens_to_ids(toks)\n",
        "    if len(ids) > max_len:\n",
        "        ids = ids[: max_len - 1] + [tokenizer.sep_token_id]\n",
        "        first_idx = [i for i in first_idx if i < len(ids)]\n",
        "        words = words[: len(first_idx)]\n",
        "    mask = [1] * len(ids)\n",
        "    inp = torch.tensor([ids], dtype=torch.long).to(device)\n",
        "    mask_t = torch.tensor([mask], dtype=torch.long).to(device)\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        preds = model(inp, mask_t)\n",
        "    pred_tags = [id2label.get(preds[0][i], \"O\") for i in first_idx]\n",
        "    # Build entity dict: TYPE -> list of phrases\n",
        "    entities = {}\n",
        "    i = 0\n",
        "    while i < len(words):\n",
        "        tag = pred_tags[i] if i < len(pred_tags) else \"O\"\n",
        "        if tag.startswith(\"B-\"):\n",
        "            entity_type = tag[2:]\n",
        "            phrase = [words[i]]\n",
        "            i += 1\n",
        "            while i < len(words) and i < len(pred_tags) and pred_tags[i] == f\"I-{entity_type}\":\n",
        "                phrase.append(words[i])\n",
        "                i += 1\n",
        "            entities.setdefault(entity_type, []).append(\" \".join(phrase))\n",
        "        else:\n",
        "            i += 1\n",
        "    return words, pred_tags, entities\n",
        "\n",
        "# Hybrid: rules for NAME/EMAIL (high recall), model for SKILL/EXPERIENCE/EDUCATION/OCCUPATION\n",
        "EMAIL_RE = re.compile(r\"[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}\", re.IGNORECASE)\n",
        "def extract_email_rules(text): return list(dict.fromkeys(EMAIL_RE.findall(text)))\n",
        "def extract_name_heuristic(text):\n",
        "    lines = [ln.strip() for ln in text.strip().split(\"\\n\") if ln.strip()]\n",
        "    for line in lines[:4]:\n",
        "        if \"@\" in line or \"http\" in line.lower() or \"www.\" in line.lower(): continue\n",
        "        parts = line.split()\n",
        "        if 1 <= len(parts) <= 4 and all(p[0].isupper() for p in parts if len(p) > 0 and p[0].isalpha()):\n",
        "            c = \" \".join(parts)\n",
        "            if len(c) < 80 and not c.endswith(\".\"): return [c]\n",
        "    return []\n",
        "def parse_resume_hybrid(text, tokenizer, model, device, id2label, max_len=512):\n",
        "    text = text.strip()\n",
        "    rn, re_ = extract_name_heuristic(text), extract_email_rules(text)\n",
        "    words, pred_tags, entities = parse_resume(text, tokenizer, model, device, id2label, max_len)\n",
        "    if rn: entities[\"NAME\"] = rn\n",
        "    if re_: entities[\"EMAIL\"] = re_\n",
        "    return words, pred_tags, entities\n",
        "\n",
        "# --- Example: set your resume text and run ---\n",
        "RESUME_TEXT = \"\"\"\n",
        "John Doe\n",
        "john.doe@email.com\n",
        "Software Engineer with 5 years of experience.\n",
        "Skills: Python, Java, Machine Learning.\n",
        "Education: BSc Computer Science, University of Colombo 2020.\n",
        "Worked at Tech Corp and Data Inc.\n",
        "\"\"\"\n",
        "\n",
        "words, tags, entities = parse_resume_hybrid(RESUME_TEXT.strip(), tokenizer, model, device, ID2LABEL)\n",
        "print(\"Entities (hybrid: NAME/EMAIL from rules, rest from model):\")\n",
        "for k, v in entities.items():\n",
        "    print(f\"  {k}: {v}\")\n",
        "print(\"\\nWord-level tags (first 30):\", list(zip(words[:30], tags[:30])))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 11) Optional: Train longer with LR scheduler\n",
        "Run this instead of cell 6 if you want to try more epochs (e.g. 25) with learning rate decay. Then run cell 7 to evaluate and cell 8 to save."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Optional: more epochs + linear LR decay (run instead of cell 6)\n",
        "EPOCHS_EXTRA = 25\n",
        "scheduler = torch.optim.lr_scheduler.LinearLR(optimizer, start_factor=1.0, end_factor=0.1, total_iters=EPOCHS_EXTRA)\n",
        "\n",
        "for epoch in range(EPOCHS_EXTRA):\n",
        "    model.train()\n",
        "    total = 0\n",
        "    for inp, mask, lab in train_loader:\n",
        "        inp, mask, lab = inp.to(device), mask.to(device), lab.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        loss = model(inp, mask, lab)\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        optimizer.step()\n",
        "        total += loss.item()\n",
        "    scheduler.step()\n",
        "    print(f\"Epoch {epoch+1}/{EPOCHS_EXTRA} Loss: {total/len(train_loader):.4f} LR: {scheduler.get_last_lr()[0]:.2e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "   EDUCATION       0.00      0.00      0.00        43\n",
            "       EMAIL       0.00      0.00      0.00        27\n",
            "  EXPERIENCE       0.00      0.00      0.00        46\n",
            "        NAME       0.00      0.00      0.00        23\n",
            "  OCCUPATION       0.00      0.00      0.00        37\n",
            "       SKILL       0.00      0.00      0.00        15\n",
            "\n",
            "   micro avg       0.00      0.00      0.00       191\n",
            "   macro avg       0.00      0.00      0.00       191\n",
            "weighted avg       0.00      0.00      0.00       191\n",
            "\n",
            "F1: 0.0\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/Users/dinalbandara/.pyenv/versions/3.12.3/lib/python3.12/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/Users/dinalbandara/.pyenv/versions/3.12.3/lib/python3.12/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ],
      "source": [
        "# 7) Evaluate with seqeval\n",
        "from seqeval.metrics import classification_report, f1_score\n",
        "\n",
        "model.eval()\n",
        "true_all, pred_all = [], []\n",
        "with torch.no_grad():\n",
        "    for inp, mask, labels in val_loader:\n",
        "        inp, mask = inp.to(device), mask.to(device)\n",
        "        preds = model(inp, mask)\n",
        "        for b in range(inp.size(0)):\n",
        "            m, labs = mask[b].cpu(), labels[b].cpu()\n",
        "            pred_b = preds[b]\n",
        "            tlist, plist = [], []\n",
        "            pos = 0\n",
        "            for i in range(m.size(0)):\n",
        "                if m[i].item()==0: break\n",
        "                p = ID2LABEL[pred_b[pos]] if pos < len(pred_b) and pred_b[pos] < NUM_LABELS else \"O\"\n",
        "                pos += 1\n",
        "                if labs[i].item() == -100: continue\n",
        "                tlist.append(ID2LABEL[labs[i].item()])\n",
        "                plist.append(p)\n",
        "            if tlist and plist:\n",
        "                true_all.append(tlist)\n",
        "                pred_all.append(plist)\n",
        "\n",
        "print(classification_report(true_all, pred_all, zero_division=0))\n",
        "val_f1 = f1_score(true_all, pred_all, zero_division=0)\n",
        "print(\"Val F1 (entity-level):\", val_f1)\n",
        "# Token-level accuracy (often higher than entity F1; 80%+ here is a good target)\n",
        "total_tok = sum(len(t) for t in true_all)\n",
        "correct_tok = sum(sum(1 for a, b in zip(t, p) if a == b) for t, p in zip(true_all, pred_all))\n",
        "token_acc = correct_tok / total_tok if total_tok else 0.0\n",
        "print(\"Token accuracy: {:.2%}\".format(token_acc))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 7b) Test on test set (formal test metrics)\n",
        "Run after cell 7. Builds a test DataLoader and reports F1/precision/recall on the **test** split (never used in training). Use this for your final reported test performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test set evaluation (same logic as cell 7, but on test_sents / test_labels)\n",
        "from seqeval.metrics import classification_report, f1_score\n",
        "\n",
        "test_ds = BertNERDataset(test_sents, test_labels, tokenizer)\n",
        "test_loader = DataLoader(test_ds, batch_size=8, collate_fn=collate)\n",
        "\n",
        "model.eval()\n",
        "true_test, pred_test = [], []\n",
        "with torch.no_grad():\n",
        "    for inp, mask, labels in test_loader:\n",
        "        inp, mask = inp.to(device), mask.to(device)\n",
        "        preds = model(inp, mask)\n",
        "        for b in range(inp.size(0)):\n",
        "            m, labs = mask[b].cpu(), labels[b].cpu()\n",
        "            pred_b = preds[b]\n",
        "            tlist, plist = [], []\n",
        "            pos = 0\n",
        "            for i in range(m.size(0)):\n",
        "                if m[i].item() == 0:\n",
        "                    break\n",
        "                p = ID2LABEL[pred_b[pos]] if pos < len(pred_b) and pred_b[pos] < NUM_LABELS else \"O\"\n",
        "                pos += 1\n",
        "                if labs[i].item() == -100:\n",
        "                    continue\n",
        "                tlist.append(ID2LABEL[labs[i].item()])\n",
        "                plist.append(p)\n",
        "            if tlist and plist:\n",
        "                true_test.append(tlist)\n",
        "                pred_test.append(plist)\n",
        "\n",
        "print(\"--- Test set results ---\")\n",
        "print(classification_report(true_test, pred_test, zero_division=0))\n",
        "print(\"Test F1:\", f1_score(true_test, pred_test, zero_division=0))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
